import kfp
import typing
from typing import Dict
from typing import NamedTuple
from kfp.v2 import dsl
from kfp.v2.dsl import (Artifact,
                        Dataset,
                        Input,
                        Model,
                        Output,
                        Metrics,
                        ClassificationMetrics,
                        component, 
                        OutputPath, 
                        InputPath)
import google.cloud.aiplatform as aip
from google_cloud_pipeline_components import aiplatform as gcc_aip
from kfp.v2.components import importer_node
from google_cloud_pipeline_components.types import artifact_types

#The Google Cloud project that this pipeline runs in.
PROJECT_ID = "your project id"
# The region that this pipeline runs in
REGION = "us-central1"
# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.
PIPELINE_ROOT = "gs://de_jads_temp"

@component(
    packages_to_install=["pandas","scikit-learn"],
    base_image="python:3.10.7-slim",
    output_component_file="train_test_split.yaml"
)
def train_test_split(dataset: Input[Dataset], dataset_train: Output[Dataset], dataset_test: Output[Dataset]):
    '''train_test_split'''
    import pandas as pd
    import logging 
    import sys
    from sklearn.model_selection import train_test_split as tts
    
    logging.basicConfig(stream=sys.stdout, level=logging.INFO) 
    
    alldata = pd.read_csv(dataset.path, index_col=None, squeeze=True)
    train, test = tts(alldata, test_size=0.3)
    train.to_csv(dataset_train.path + ".csv" , index=False, encoding='utf-8-sig')
    test.to_csv(dataset_test.path + ".csv" , index=False, encoding='utf-8-sig')
	
	
@component(
    packages_to_install = [
        "pandas",
        "sklearn",
        "google-cloud-aiplatform",
    ], base_image="python:3.10.7-slim",  output_component_file="model_evaluation.yaml"
)
def lr_model_evaluation(
    project_id: str,
    region:str,
    test_set:  Input[Dataset],
    model_lr: Input[Model],
    thresholds_dict_str: str,
    metrics: Output[ClassificationMetrics],
    kpi: Output[Metrics]
) -> NamedTuple("output", [("approval", str)]):

    import pandas as pd
    import logging     
    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score
    import json
    import typing
    import pickle
    from google.cloud import aiplatform
    import sklearn.metrics as skl_metrics
    from datetime import datetime
    import random
    idn = random.randint(0,1000)
    
    def threshold_check(val1, val2):
        cond = "false"
        if val1 >= val2 :
            cond = "true"
        return cond

    data = pd.read_csv(test_set.path+".csv")
    
    #Loading the saved model
    m_filename = model_lr.path + ".pkl"
    model = pickle.load(open(m_filename, 'rb'))
    
    y_test = data.drop(columns=["class"])
    y_target = data['class']
    y_pred = model.predict(y_test)    

    y_scores =  model.predict_proba(data.drop(columns=["class"]))[:, 1]
    fpr, tpr, thresholds = roc_curve(
         y_true=data['class'].to_numpy(), y_score=y_scores, pos_label=True
    )
    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())  
    
    metrics.log_confusion_matrix(
       ["False", "True"],
       confusion_matrix(
           data['class'], y_pred
       ).tolist(), 
    ) 
      
    accuracy = accuracy_score(data['class'], y_pred.round())
    thresholds_dict  = json.loads(thresholds_dict_str)
    model_lr.metadata["accuracy"] = float(accuracy)
    kpi.log_metric("accuracy", float(accuracy))
    approval = threshold_check(float(accuracy), int(thresholds_dict['roc']))
    
    # How to start an expriment - just for demonstration 
    aiplatform.init(
       project=project_id,
       location=region,
       experiment="lrdiabetes"
    )
    
    run_id = f"run-{idn}-{datetime.now().strftime('%Y%m%d%H%M%S')}"
    aiplatform.start_run(run_id)
    
    training_metrics = {
        'model_accuracy': skl_metrics.accuracy_score(y_target, y_pred),
        'model_precision': skl_metrics.precision_score(y_target, y_pred, average='macro'),
        'model_recall': skl_metrics.recall_score(y_target, y_pred, average='macro'),
        'model_logloss': skl_metrics.log_loss(y_target, y_pred),
        'model_auc_roc': skl_metrics.roc_auc_score(y_target, y_pred)
    }
    aiplatform.log_metrics(training_metrics)
    
    return (approval,)
	
	
@component(
    packages_to_install=["google-cloud-storage"],
    base_image="python:3.10.7-slim",
    output_component_file="model_upload_gsc.yaml"
)
def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):
    '''upload model to gsc bucket'''
    from google.cloud import storage   
    import logging 
    import sys
    
    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    
  
    # upload the model to GCS
    client = storage.Client(project=project_id)
    bucket = client.get_bucket(model_repo)
    blob = bucket.blob('model.pkl') 
    blob.upload_from_filename(model.path + '.pkl')   
    
    
    print("Saved the model to GCP bucket : " + model_repo)
	
@component(
    packages_to_install=["google-cloud-storage"],
    base_image="python:3.10.7-slim",
    output_component_file="model_upload_gsc.yaml"
)
def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):
    '''upload model to gsc bucket'''
    from google.cloud import storage   
    import logging 
    import sys
    
    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    
  
    # upload the model to GCS
    client = storage.Client(project=project_id)
    bucket = client.get_bucket(model_repo)
    blob = bucket.blob('model.pkl') 
    blob.upload_from_filename(model.path + '.pkl')   
    
    
    print("Saved the model to GCP bucket : " + model_repo)
	
from kfp.v2 import compiler
compiler.Compiler().compile(pipeline_func=pipeline,
        package_path='diabetes_prdictor_training_pipeline_lab5ex1.json')

job = aip.PipelineJob(
    display_name="diabetes-predictor-lab5ex1",
    enable_caching=False,
    template_path="diabetes_prdictor_training_pipeline_lab5ex1.json",
    pipeline_root=PIPELINE_ROOT,
    location=REGION,
    parameter_values={
        'project_id': PROJECT_ID, # makesure to use your project id,
        'region': REGION,
        'data_bucket': 'data_de2022',  # makesure to use your data bucket name 
        'dataset_uri':'gs://data_de2022/training_set.csv',
        'model_repo':'model_repo_de2022', # makesure to use your model bucket name 
        'thresholds_dict_str':'{"roc":0.8}'
    }
)

job.run()