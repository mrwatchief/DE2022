{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bf0613-d6ec-4cf4-87e5-062fd3bd3a82",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611aeefc-f5ce-40e8-acc7-d04ac84da39a",
   "metadata": {},
   "source": [
    "## Some of the source codes are based on\n",
    "https://towardsdatascience.com/how-to-set-up-custom-vertex-ai-pipelines-step-by-step-467487f81cad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f1d825-84cc-43ac-9fe2-f204d77f0962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.14 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\n",
      "google-cloud-pipeline-components 1.0.25 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc6a21-604f-4a52-b904-e3bb18a61b2f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52dad0c4-c173-46b8-bf99-d6e8efc35316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2207b06-771f-4dbb-a713-90c50745c0ea",
   "metadata": {},
   "source": [
    "Check the versions of the packages you installed. The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b60838-e5a2-41cd-ae93-43925343fba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.14\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0bcff2-3ffb-4e51-b852-511cb10ad0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2.components import importer_node\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afffb0-449b-4669-807a-793f526277fe",
   "metadata": {},
   "source": [
    "#### Project and Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf6aad4-f675-47aa-820b-14daa796d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"yourprojectid\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://de_jads_temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bace39-57ba-49ee-bd74-9eaf4093f471",
   "metadata": {},
   "source": [
    "#### Create Pipeline Components\n",
    "\n",
    "We can create a component from Python functions (inline) and from a container. We will first try inline python functions. \n",
    "\n",
    "Step 1: Define the python function\n",
    "\n",
    "Step 2:  Use **kfp.components.create_component_from_func** build the component. This function takes four parameters.\n",
    "\n",
    "**1.func**: The Python function to convert.\n",
    "\n",
    "**2.base_image**: (Optional.) Specify the Docker container image to run this function in. \n",
    "\n",
    "**3.output_component_file**: (Optional.) Writes your component definition to a file. \n",
    "\n",
    "**4.packages_to_install**: (Optional.) A list of versioned Python packages to install before running your function.\n",
    "\n",
    "Another thing we need to consider is passing parameters between components. We can pass simple parameters such as integer, string, tuple, dict, and list by values. To pass the large datasets or complex configurations, we can use files. We can annotate the Python functionâ€™s parameters to indicate input or output files for the component. \n",
    "\n",
    "Refer to  https://www.kubeflow.org/docs/components/pipelines/sdk/python-function-components/ for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457ef88-cd95-4304-b6e0-143b718c44aa",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bc305f-2456-4c07-b89f-427b0f24eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\",\"scikit-learn\"],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    "    output_component_file=\"train_test_splitv2.yaml\"\n",
    ")\n",
    "def train_test_split(dataset: Input[Dataset], dataset_train: Output[Dataset], dataset_test: Output[Dataset]):\n",
    "    '''train_test_split'''\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO) \n",
    "    \n",
    "    alldata = pd.read_csv(dataset.path, index_col=None, squeeze=True)\n",
    "    train, test = tts(alldata, test_size=0.3)\n",
    "    train.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911d312-549c-4be7-bef0-e02c9d8cf80f",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acee72f0-007a-4e9c-853a-e6cf66f2a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=['pandas','scikit-learn'],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    "    output_component_file=\"train_lr_modelv2.yaml\"\n",
    ")\n",
    "def train_lr (features: Input[Dataset], model: Output[Model]):\n",
    "    '''train a LogisticRegression with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression        \n",
    "    import pickle \n",
    "    \n",
    "    data = pd.read_csv(features.path+\".csv\")\n",
    "    model_lr = LogisticRegression()\n",
    "    model_lr.fit(data.drop('class',axis=1), data['class'])\n",
    "    model.metadata[\"framework\"] = \"LR\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(model_lr, file)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0f7f9-6e07-40e4-a220-cb2ccfe76337",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872f6eea-d4ab-4267-80de-8fa9dd954ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "    ], base_image=\"python:3.10.7-slim\",  output_component_file=\"model_evaluationv2.yaml\"\n",
    ")\n",
    "def lr_model_evaluation(\n",
    "    test_set:  Input[Dataset],\n",
    "    model_lr: Input[Model],\n",
    "    thresholds_dict_str: str,\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    kpi: Output[Metrics]\n",
    ") -> NamedTuple(\"output\", [(\"approval\", str)]):\n",
    "  \n",
    "    import pandas as pd\n",
    "    import logging     \n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score\n",
    "    import json\n",
    "    import typing\n",
    "    import pickle\n",
    "    \n",
    "    def threshold_check(val1, val2):\n",
    "        cond = \"false\"\n",
    "        if val1 >= val2 :\n",
    "            cond = \"true\"\n",
    "        return cond\n",
    "\n",
    "    data = pd.read_csv(test_set.path+\".csv\")\n",
    "    \n",
    "     #Loading the saved model with joblib\n",
    "    m_filename = model_lr.path + \".pkl\"\n",
    "    model = pickle.load(open(m_filename, 'rb'))\n",
    "    \n",
    "    y_test = data.drop(columns=[\"class\"])\n",
    "    y_target = data['class']\n",
    "    y_pred = model.predict(y_test)    \n",
    "\n",
    "    y_scores =  model.predict_proba(data.drop(columns=[\"class\"]))[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(\n",
    "         y_true=data['class'].to_numpy(), y_score=y_scores, pos_label=True\n",
    "    )\n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())  \n",
    "    \n",
    "    metrics.log_confusion_matrix(\n",
    "       [\"False\", \"True\"],\n",
    "       confusion_matrix(\n",
    "           data['class'], y_pred\n",
    "       ).tolist(), \n",
    "    )\n",
    "    \n",
    "    accuracy = accuracy_score(data['class'], y_pred.round())\n",
    "    thresholds_dict  = json.loads(thresholds_dict_str)\n",
    "    model_lr.metadata[\"accuracy\"] = float(accuracy)\n",
    "    kpi.log_metric(\"accuracy\", float(accuracy))\n",
    "    approval = threshold_check(float(accuracy), int(thresholds_dict['roc']))\n",
    "    return (approval,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa41f3c-110d-4bde-bd99-22fcec229930",
   "metadata": {},
   "source": [
    "### Upload Model and Metrics to Google Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84923d8-2952-4ea0-8118-dc0ce7822f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    "    output_component_file=\"model_uploadv2.yaml\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):\n",
    "    '''upload model to gsc'''\n",
    "    from google.cloud import storage   \n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('model.pkl') \n",
    "    blob.upload_from_filename(model.path + '.pkl')   \n",
    "    \n",
    "    \n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce80fda-8515-4f8b-afe0-459aa1f4fede",
   "metadata": {},
   "source": [
    "### Deploy the model at Vertext AI \n",
    "We can use Google Pre-built Kebeflow componets such as  EndpointCreateOp, ModelUploadOp, and ModelDeployOp to deploy the models locally at Vertex AI.\n",
    "\n",
    "***This is only for testing.  In your assigment, please use custom serving applications and CI-CD pipelines to deploy models. We should be able to deploy a given model at any given production environment. CI-CD pipelines are the best solution. ***\n",
    "\n",
    "<img src=\"imgs/EnterpriseMlOps_Model_Deployment.png\">\n",
    "\n",
    "source: https://cloud.redhat.com/blog/enterprise-mlops-reference-design\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/pipelines/gcpc-list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131e73b-f997-4676-ac33-8f33c10a1a8d",
   "metadata": {},
   "source": [
    "### Enable Artifact Registryy API\n",
    "\n",
    "https://cloud.google.com/artifact-registry/docs/enable-service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166590b3-f788-4e4c-8e31-fb981da56966",
   "metadata": {},
   "source": [
    "#### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a96b6ae0-234b-4883-ae95-8599689a5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"diabetes-prdictor-training-pipeline-lab5ex2\")\n",
    "def pipeline(project_id: str, data_bucket: str, dataset_uri: str, model_repo: str, thresholds_dict_str:str):    \n",
    "    \n",
    "    dataset_op = kfp.dsl.importer(\n",
    "        artifact_uri=dataset_uri,\n",
    "        artifact_class=Dataset,\n",
    "        reimport=False,\n",
    "    )\n",
    "     \n",
    "    train_test_split_op = train_test_split(dataset_op.output)\n",
    "        \n",
    "    training_lr_job_run_op = train_lr(features=train_test_split_op.outputs[\"dataset_train\"])\n",
    "    \n",
    "    model_evaluation_op = lr_model_evaluation(\n",
    "        test_set=train_test_split_op.outputs[\"dataset_test\"],\n",
    "        model_lr=training_lr_job_run_op.outputs[\"model\"],\n",
    "        thresholds_dict_str = thresholds_dict_str, # I deploy the model anly if the model performance is above the threshold\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        model_evaluation_op.outputs[\"approval\"]==\"true\",\n",
    "        name=\"approve-model\",\n",
    "    ):\n",
    "        upload_model_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_lr_job_run_op.outputs['model']\n",
    "        )    \n",
    "        \n",
    "        import_unmanaged_model_task = importer_node.importer(\n",
    "            artifact_uri= \"gs://model_repo_de2022\",\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",  # see https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers  \n",
    "                },\n",
    "            },\n",
    "        ).after(upload_model_to_gc_op)      \n",
    "       \n",
    "        # using Google's custom components for for uloading and deploying the model.\n",
    "       \n",
    "        model_upload_op = gcc_aip.ModelUploadOp(\n",
    "            project=project_id,\n",
    "            display_name=\"diabetes-prediction-model\",\n",
    "            unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "        ).after(import_unmanaged_model_task)       \n",
    "               \n",
    "        create_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            display_name=\"diabetes-prediction-service\",\n",
    "        ).after(model_upload_op)      \n",
    "        \n",
    "        model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "            deployed_model_display_name=\"diabetes-prediction-model\",\n",
    "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            traffic_split={\"0\": 100},\n",
    "        ).after(create_endpoint_op)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac278200-c580-4f40-bc8b-1817d3b13c13",
   "metadata": {},
   "source": [
    "#### Compile the pipeline into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ee4b21-89e6-4f63-845c-b249556ea919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/dsl/__init__.py:32: FutureWarning: `kfp.dsl.importer` is a deprecated alias and will be removed in KFP v2.0. Please import from `kfp.v2.dsl` instead.\n",
      "  category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='diabetes_prdictor_training_pipeline_lab5ex2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87025e-08d7-4608-b37d-c929b6eb5a3c",
   "metadata": {},
   "source": [
    "#### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83b88e89-42cd-4e64-bc4e-8e3eddebccff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/958343845263/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/958343845263/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203?project=958343845263\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/958343845263/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-lab5ex1-20221014080203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"diabetes-predictor-lab5ex2\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"diabetes_prdictor_training_pipeline_lab5ex2.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID, # makesure to use your project id \n",
    "        'data_bucket': 'data_de2022',  # makesure to use your data bucket name \n",
    "        'dataset_uri':'gs://data_de2022/training_set.csv',\n",
    "        'model_repo':'model_repo_de2022', # makesure to use your model bucket name \n",
    "        'thresholds_dict_str':'{\"roc\":0.8}'\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39b3ee-f5f6-4b48-bad3-8544855ed3ed",
   "metadata": {},
   "source": [
    "### List all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d3e31cd-1964-4d6a-9a9e-69ae90ea945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "MODEL_ID             DISPLAY_NAME\n",
      "2643010448894459904  diabetes-prediction-model\n",
      "8233103486368088064  diabetes-prediction-model\n",
      "8915398829914718208  diabetes-prediction-model\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"diabetes-prediction-model\"\n",
    "! gcloud ai models list --region={REGION} --filter={DISPLAY_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf9a14-3010-4568-8348-eeb168a22e97",
   "metadata": {},
   "source": [
    "### Make an online prediction request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c68681-17cc-4d9f-8ce3-568f48f93cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[0.0], deployed_model_id='1654567087709880320', model_version_id='1', model_resource_name='projects/958343845263/locations/us-central1/models/8915398829914718208', explanations=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENDPOINT_NAME=\"diabetes-prediction-service\"\n",
    "instance = [[1,126,60,0,0,30.1,0.349,47]]  # Prediciton request inputs \n",
    "ENDPOINT_ID = !(gcloud ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)'\\\n",
    "              --filter=display_name=$ENDPOINT_NAME \\\n",
    "              --sort-by=creationTimeStamp | tail -1)\n",
    "ENDPOINT_ID = ENDPOINT_ID[1]\n",
    "\n",
    "def endpoint_predict(\n",
    "    project: str, location: str, instances: list, endpoint: str\n",
    "):\n",
    "    aip.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aip.Endpoint(endpoint)\n",
    "\n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    return prediction\n",
    "\n",
    "endpoint_predict(PROJECT_ID, REGION, instance, ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a5727-af46-44af-a44f-2c555940410f",
   "metadata": {},
   "source": [
    "### Test the batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7dcd4a-c3b7-4d29-8ba6-68c696962fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n",
      "BatchPredictionJob created. Resource name: projects/958343845263/locations/us-central1/batchPredictionJobs/4075980588752830464\n",
      "To use this BatchPredictionJob in another session:\n",
      "bpj = aiplatform.BatchPredictionJob('projects/958343845263/locations/us-central1/batchPredictionJobs/4075980588752830464')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/4075980588752830464?project=958343845263\n",
      "BatchPredictionJob projects/958343845263/locations/us-central1/batchPredictionJobs/4075980588752830464 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/958343845263/locations/us-central1/batchPredictionJobs/4075980588752830464 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/958343845263/locations/us-central1/batchPredictionJobs/4075980588752830464 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/958343845263/locations/us-central1/batchPredictionJobs/4075980588752830464 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/958343845263/locations/us-central1/batchPredictionJobs/4075980588752830464 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/958343845263/locations/us-central1/batchPredictionJobs/4075980588752830464 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "# Define variables \n",
    "job_display_name = \"diabetes-prediction-batch-prediction-job\"\n",
    "MODEL_NAME=\"diabetes-prediction-model\"\n",
    "ENDPOINT_NAME=\"diabetes-prediction-service\"\n",
    "BUCKET_URI=\"gs://data_de2022\"\n",
    "input_file_name=\"test_set_batch_prediction.csv\"\n",
    "\n",
    "# Get model id\n",
    "MODEL_ID=!(gcloud ai models list --region=$REGION \\\n",
    "           --filter=display_name=$MODEL_NAME)\n",
    "MODEL_ID=MODEL_ID[2].split(\" \")[0]\n",
    "\n",
    "model_resource_name = f'projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}'\n",
    "gcs_source= [f\"{BUCKET_URI}/{input_file_name}\"]\n",
    "gcs_destination_prefix=f\"{BUCKET_URI}/output\"\n",
    "\n",
    "def batch_prediction_job(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_resource_name: str,\n",
    "    job_display_name: str,\n",
    "    gcs_source: str,\n",
    "    gcs_destination_prefix: str,\n",
    "    machine_type: str,\n",
    "    starting_replica_count: int = 1, # The number of nodes for this batch prediction job. \n",
    "    max_replica_count: int = 1,    \n",
    "):   \n",
    "    aip.init(project=project, location=location)\n",
    "\n",
    "    model = aip.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        instances_format='csv', #csv\n",
    "        gcs_source=[f\"{BUCKET_URI}/{input_file_name}\"],\n",
    "        gcs_destination_prefix=f\"{BUCKET_URI}/output\",\n",
    "        machine_type=machine_type, # must be present      \n",
    "    )\n",
    "    batch_prediction_job.wait()\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job\n",
    "\n",
    "batch_prediction_job(PROJECT_ID, REGION, model_resource_name, job_display_name, gcs_source, gcs_destination_prefix, machine_type=\"n1-standard-2\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m84"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
