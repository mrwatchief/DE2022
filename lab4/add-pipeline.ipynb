{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1215351f-10dc-4af6-9045-33187120207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.14 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\n",
      "google-cloud-pipeline-components 1.0.24 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f0bcff2-3ffb-4e51-b852-511cb10ad0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afffb0-449b-4669-807a-793f526277fe",
   "metadata": {},
   "source": [
    "#### Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf6aad4-f675-47aa-820b-14daa796d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "project_id = \"your projectid\"\n",
    "# The region that this pipeline runs in\n",
    "region = \"us-west1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "pipeline_root_path = \"your GCS bucket for pipeline root\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bace39-57ba-49ee-bd74-9eaf4093f471",
   "metadata": {},
   "source": [
    "#### Create Pipeline Components\n",
    "\n",
    "We can create a component from Python functions (inline) and from a container. We will first try inline python functions. \n",
    "\n",
    "Step 1: Define the python function\n",
    "\n",
    "Step 2:  Use **kfp.components.create_component_from_func** build the component. This function takes four parameters.\n",
    "\n",
    "**1.func**: The Python function to convert.\n",
    "\n",
    "**2.base_image**: (Optional.) Specify the Docker container image to run this function in. \n",
    "\n",
    "**3.output_component_file**: (Optional.) Writes your component definition to a file. \n",
    "\n",
    "**4.packages_to_install**: (Optional.) A list of versioned Python packages to install before running your function.\n",
    "\n",
    "Another thing we need to consider is passing parameters between components. We can pass simple parameters such as integer, string, tuple, dict, and list by values. To pass the large datasets or complex configurations, we can use files. We can annotate the Python functionâ€™s parameters to indicate input or output files for the component. \n",
    "\n",
    "Refer to  https://www.kubeflow.org/docs/components/pipelines/sdk/python-function-components/ for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457ef88-cd95-4304-b6e0-143b718c44aa",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bc305f-2456-4c07-b89f-427b0f24eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def add(a: float, b: float) -> float:\n",
    "  '''Calculates sum of two arguments'''\n",
    "  return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32942c5-9171-4724-b8f7-e5fb7ab6a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='addition-pipeline',\n",
    "  description='An example pipeline that performs addition calculations.',\n",
    "  pipeline_root='gs://my-pipeline-root/example-pipeline'\n",
    ")\n",
    "def add_pipeline(\n",
    "  a: float=1,\n",
    "  b: float=7,\n",
    "):\n",
    "  # Passes a pipeline parameter and a constant value to the `add` factory\n",
    "  # function.\n",
    "  first_add_task = add(a, 4)\n",
    "  # Passes an output reference from `first_add_task` and a pipeline parameter\n",
    "  # to the `add` factory function. For operations with a single return\n",
    "  # value, the output reference can be accessed as `task.output` or\n",
    "  # `task.outputs['output_name']`.\n",
    "  second_add_task = add(first_add_task.output, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac278200-c580-4f40-bc8b-1817d3b13c13",
   "metadata": {},
   "source": [
    "#### Compile the pipeline into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ee4b21-89e6-4f63-845c-b249556ea919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=add_pipeline,\n",
    "        package_path='add_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87025e-08d7-4608-b37d-c929b6eb5a3c",
   "metadata": {},
   "source": [
    "#### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b88e89-42cd-4e64-bc4e-8e3eddebccff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/958343845263/locations/us-central1/pipelineJobs/addition-pipeline-20221004084116\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/958343845263/locations/us-central1/pipelineJobs/addition-pipeline-20221004084116')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/addition-pipeline-20221004084116?project=958343845263\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/addition-pipeline-20221004084116 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/addition-pipeline-20221004084116 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/addition-pipeline-20221004084116 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/addition-pipeline-20221004084116 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/958343845263/locations/us-central1/pipelineJobs/addition-pipeline-20221004084116 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/958343845263/locations/us-central1/pipelineJobs/addition-pipeline-20221004084116\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"add-pipeline\",\n",
    "    template_path=\"add_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'a':8,\n",
    "        'b': 9\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m84"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
